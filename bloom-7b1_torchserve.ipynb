{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86394b2f-f768-444f-a6c5-356abd173cef",
   "metadata": {},
   "source": [
    "# Pytorch Serve\n",
    "\n",
    "This tutorial assumes that you already have knowledge of the basic concepts of pytorch serve. If you have no the aspect knowledge, please see https://pytorch.org/serve/ website.\n",
    "\n",
    "This tutorial will take you to do bloom-7b1 model inference. Please see https://huggingface.co/bigscience/bloom-7b1 to know more bloom-7b1 model. \n",
    "There is mainly 3 sections to learn how to do model inference.\n",
    "* Load large Huggingface models with constrained resources using accelerate\n",
    "* Start model with torchserve\n",
    "* Run model inference to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56784f41-931f-4956-9fb3-5f0176545abe",
   "metadata": {},
   "source": [
    "## 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f365efa0-9846-4c08-b306-7a81b9614125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=http://proxy.vmware.com:3128\n",
    "!export https_proxy=http://proxy.vmware.com:3128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5f5055-fe3f-48ef-8555-4510ef35df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f2bddffa940>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e7/87/25dd46811431cfc5e8d6ba8c80758cb3131574b271fbf06cf1b691dba8d4/accelerate-0.18.0-py3-none-any.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m31.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
      "Installing collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config and accelerate-launch are installed in '/home/model-server/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-0.18.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pynvml==8.0.4\n",
      "  Downloading pynvml-8.0.4-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: pynvml\n",
      "Successfully installed pynvml-8.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "\n",
    "# fix: pynvml.nvml.NVMLError_FunctionNotFound when start bloom-7b1 model\n",
    "!pip install pynvml==8.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cc870-3288-40b7-b01e-27b369ab8a94",
   "metadata": {},
   "source": [
    "## 1. Prepare model and configurations\n",
    "We have to prepare model MAR file and configuration (config.properties) to start model. Thus, This step will guide you through this process.\n",
    "### 1.1 Download bloom-7b1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6d6e1-dbd5-4236-b573-43b8deb4acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Download_model.py --model_name bigscience/bloom-7b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e1a7e-0b3e-48d6-9ba0-2dcd285314d9",
   "metadata": {},
   "source": [
    "The script prints the path where the model is downloaded as below.\n",
    "\n",
    "model/models--bigscience-bloom-7b1/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/\n",
    "\n",
    "The downloaded model is around 14GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f6d2a-e0ac-4bf2-8bd6-e431bc7306b9",
   "metadata": {},
   "source": [
    "### 1.2 Compress downloaded model\n",
    "\n",
    "Navigate to the path got from the above script. In this example it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb44cd-c4b4-4fdc-aa79-fe58fba72d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd model/models--bigscience-bloom-7b1/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/\n",
    "zip -r ~/model.zip *\n",
    "cd ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf270e2-11aa-4f25-87b8-bb7987d6ae0b",
   "metadata": {},
   "source": [
    "### 1.3 Generate MAR file\n",
    "\n",
    "Navigate up to the directory that have custome_handler.py, model.zip, setup_config.json.\n",
    "* custom_handler.py: codes for model initialization, pre-processing, post-processing, etc.\n",
    "* model.zip: Compressed package of model files (*.bin) should be the checkpoint.\n",
    "* setup_config.json: configurations when loading large huggingface model, Refer: https://huggingface.co/docs/transformers/main_classes/model#large-model-loading\n",
    "\n",
    "**Notice**: Should update parameters in the setup_config.json file according to your device resources. \n",
    "For example, With device_map=\"sequential\", Huggingface accelerate will occupy gpu memory in the order of GPUs; \n",
    "\"max_memory\": {\n",
    "        \"0\": \"32GB\"\n",
    "    }\n",
    "There is only 32GB gpu card, hope to load model into this GPU card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4efebb6-b15b-439f-b17f-a6ae887097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver --model-name bloom --version 1.0 --handler custom_handler.py --extra-files model.zip,setup_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03eb10a-99bf-4d58-9f5f-b597a2d59c27",
   "metadata": {},
   "source": [
    "You will see the bloom.mar file once the process executed finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0e66b-fd73-4073-ab5a-4978c086727c",
   "metadata": {},
   "source": [
    "## 2. Start model with torchserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff430dd0-0868-4363-9c73-8e465ac7facb",
   "metadata": {},
   "source": [
    "Move the Mar file to the specific directory\n",
    "\n",
    "Update config.properties, especially notice model_store directory that need to find the MAR file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc7baab-583c-4fd9-aab3-c99d60bbead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchserve/model_store/*.mar\n",
    "!mv bloom.mar torchserve/model_store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b11e63c-1479-40e7-9b9f-19d6e295e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting torchserve/config/config.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile torchserve/config/config.properties\n",
    "inference_address=http://0.0.0.0:8080\n",
    "management_address=http://0.0.0.0:8081\n",
    "metrics_address=http://0.0.0.0:8082\n",
    "enable_envvars_config=true\n",
    "install_py_dep_per_model=true\n",
    "number_of_gpu=1\n",
    "load_models=all\n",
    "max_response_size=655350000\n",
    "default_response_timeout=6000\n",
    "model_store=/home/model-server/bloom/torchserve/model_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33aa0175-5ef3-4685-a9f8-c9cd95b06caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3041736\n",
      "drwxr-xr-x 2 model-server model-server       4096 Apr 18 02:13 .\n",
      "drwxr-xr-x 5 model-server model-server       4096 Apr 18 02:12 ..\n",
      "-rw-r--r-- 1 model-server model-server 3114721892 Apr 18 02:06 bloom.mar\n",
      "total 16\n",
      "drwxr-xr-x 3 model-server model-server 4096 Apr 18 02:10 .\n",
      "drwxr-xr-x 5 model-server model-server 4096 Apr 18 02:12 ..\n",
      "drwxr-xr-x 2 model-server model-server 4096 Apr 18 02:10 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 model-server model-server  320 Apr 18 02:12 config.properties\n"
     ]
    }
   ],
   "source": [
    "!ls -al torchserve/model_store/\n",
    "!ls -al torchserve/config/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfba46-ce96-4a76-b662-e9e7b51dde52",
   "metadata": {},
   "source": [
    "**It takes about 2 to 5 minutes to start model, that depends to your device resources.**\n",
    "\n",
    "If you want to see the logs in the real time, you can run the below command in the terminal. and if you run the below command to start model in this notebook server, that might can't see the logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1db478d-ae08-4a29-8ab0-dccdd4996ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchServe is already running, please use torchserve --stop to stop TorchServe.\n"
     ]
    }
   ],
   "source": [
    "!torchserve --start --ncs --ts-config ./torchserve/config/config.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef5213-64d0-4eae-a3ff-416e6e9a371c",
   "metadata": {},
   "source": [
    "## 3. Run model inference to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "800e9dad-d161-4128-b6bf-4779b8afb6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8080...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8080 (#0)\n",
      "> PUT /predictions/bloom HTTP/1.1\n",
      "> Host: localhost:8080\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> Content-Length: 54\n",
      "> Expect: 100-continue\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 100 Continue\n",
      "* We are completely uploaded and fine\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 \n",
      "< x-request-id: 8af411b9-1ca8-4a6f-930d-124df17f9c2e\n",
      "< Pragma: no-cache\n",
      "< Cache-Control: no-cache; no-store, must-revalidate, private\n",
      "< Expires: Thu, 01 Jan 1970 00:00:00 UTC\n",
      "< content-length: 399\n",
      "< connection: keep-alive\n",
      "< \n",
      "Today the weather is really nice and I am planning on\n",
      "getting some new clothes for myself and my friends. The house is already filled with\n",
      "various things from my mom's. I have the costume to go with that, as well.\n",
      "But I just wanted to go on vacation with these costumes. It will be fun to\n",
      "wear them whenever I want to. For the most part we have to wait around\n",
      "* Connection #0 to host localhost left intact\n",
      "until the beginning of the summer, just"
     ]
    }
   ],
   "source": [
    "!curl -v \"http://localhost:8080/predictions/bloom\" -T sample_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0338151-695f-4009-905c-a28b2acd5321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
